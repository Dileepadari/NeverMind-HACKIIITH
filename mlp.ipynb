{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   beauty  books  clothing  electronics  food  furniture  home  sports\n",
      "0       0      0         1            1     0          0     0       1\n",
      "1       1      0         0            0     0          1     0       0\n",
      "2       0      0         1            1     1          0     0       1\n",
      "3       0      0         0            0     1          0     0       0\n",
      "4       0      0         0            0     0          0     1       0\n",
      "5       0      1         0            1     0          0     0       1\n",
      "6       1      0         1            0     0          1     0       0\n",
      "7       1      1         0            0     0          0     0       0\n",
      "8       0      0         0            1     1          0     1       1\n",
      "9       0      0         0            0     1          1     0       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('advertisement.csv')\n",
    "\n",
    "# Split features and labels\n",
    "X = data.drop(columns='labels')\n",
    "y = data['labels'].str.get_dummies(sep=' ')  # Create a binary indicator for each class\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class MLP:\n",
    "#     def __init__(self,input_size,output_size,lr,epoch,act_fn,opt_fn,n_hidden,n_neuron,batch_size,\n",
    "#                  type = 'classification'\n",
    "#                  ):\n",
    "#         self.lr = lr\n",
    "#         self.epoch = epoch\n",
    "#         self.act_fn = act_fn\n",
    "#         self.opt_fn = opt_fn\n",
    "#         self.n_hidden = n_hidden\n",
    "#         self.n_neuron = n_neuron\n",
    "#         self.batch_size = batch_size\n",
    "#         self.layer_sizes = input_size + n_neuron + output_size \n",
    "#         self.weights,self.bias = self.init_params()\n",
    "#         self.type = type\n",
    "    \n",
    "#     def init_params(self):\n",
    "#         # initialise the params for all layers of weights and bias:\n",
    "#         weights = []\n",
    "#         bias = []\n",
    "#         for i in range(1,len(self.layer_sizes)):\n",
    "#             weights.append(np.random.rand(self.n_neuron[i-1],self.n_neuron[i]))\n",
    "#             bias.append(np.zeros((self.n_neuron[i],1)))\n",
    "#         return weights,bias\n",
    "\n",
    "        \n",
    "\n",
    "#     def activation_fn(self,act_fn,z):\n",
    "#         if act_fn == 'linear':\n",
    "#             return z\n",
    "#         elif act_fn == 'sigmoid':\n",
    "#             return 1/(1+np.exp(-z))\n",
    "#         elif act_fn == 'relu':\n",
    "#             return np.maximum(z,0)\n",
    "#         elif act_fn == 'tanh':\n",
    "#             return np.tanh(z)\n",
    "\n",
    "#     def output_cal(self,z):\n",
    "#         if self.type == 'classification':\n",
    "#             return self.activation_fn('sigmoid',z)\n",
    "#         elif self.type == 'regression':\n",
    "#             return z\n",
    "\n",
    "\n",
    "#     def forward(self,X):\n",
    "#         a_list = []\n",
    "#         z_list = []\n",
    "#         a = X.T\n",
    "#         for i in range(self.n_hidden):\n",
    "#             z = np.dot(self.weights[i],a) + self.bias[i]\n",
    "#             a = self.activation_fn(self.act_fn,z)\n",
    "#             a_list.append(a)\n",
    "#             z_list.append(z)\n",
    "\n",
    "#         z = np.dot(self.weights[-1],a) + self.bias[-1]\n",
    "#         # output layer\n",
    "#         y_hat = self.output_cal(z)\n",
    "#         a_list.append(y_hat)\n",
    "#         z_list.append(z)\n",
    "#         return z_list,a_list\n",
    "\n",
    "\n",
    "#     def backpropagation(self,X,y,z_list,a_list,loss):\n",
    "#         # assume L is the loss:\n",
    "#         gradient_w = []\n",
    "#         gradient_b = []\n",
    "\n",
    "#         # output layer\n",
    "#         delta = self.output_act_derivative(z_list[-1])*loss\n",
    "#         dE_dw = np.dot(delta,a_list[-2].T)/X.shape[0]\n",
    "#         gradient_w.append(dE_dw)\n",
    "#         dE_db = np.sum(delta,axis=0,keepdims=True)/X.shape[0]\n",
    "#         gradient_b.append(dE_db)\n",
    "\n",
    "\n",
    "\n",
    "#         #other layers:\n",
    "#         for i in range(self.n_hidden-1,-1,-1):\n",
    "#             # i -> n_hidden-1 to 0\n",
    "#             delta = self.activation_fn_derivative(z_list[i])*np.dot(self.weights[i+1],delta)\n",
    "#             if i > 0:\n",
    "#                 dE_dw = np.dot(delta,a_list[i-1].T)\n",
    "#             else:\n",
    "#                 dE_dw = np.dot(delta,X.T)\n",
    "#             dE_dw /= X.shape[0]\n",
    "#             gradient_w.append(dE_dw)\n",
    "#             dE_db = np.sum(delta,axis=1,keepdims=True)/X.shape[0]\n",
    "#             gradient_b.append(dE_db)\n",
    "\n",
    "#         return gradient_w.reverse(),gradient_b.reverse()\n",
    "\n",
    "\n",
    "#     def optimizer(self):\n",
    "#         # update weights and bias\n",
    "#         if self.opt == 'SGD':\n",
    "#             self.sdg_optimizer()\n",
    "#         elif self.opt_fn == 'BGD':\n",
    "#             self.batch_optimizer()\n",
    "#         else:\n",
    "#             self.mini_batch_optimizer()\n",
    "\n",
    "#     def batch_optimizer(self,X,y):\n",
    "#         # batch gradient descent\n",
    "#         z,a = self.forward(X,y)\n",
    "#         loss = self.loss_fn(y,a[-1])\n",
    "#         gradient_w,gradient_b = self.backpropagation(X,y,z,a,loss)\n",
    "\n",
    "#     def sdg_optimizer(self,X,y):\n",
    "#         # stochastic gradient descent\n",
    "#         for i in range(X.shape[0]):\n",
    "#             z,a = self.forward_pass(X[i],y[i])\n",
    "#             loss = self.loss_fn(y[i],z[-1])\n",
    "#             gradient_w,gradient_b = self.backpropagation(X[i],y[i],z,a,loss)\n",
    "            \n",
    "#     def mini_batch_optimizer(self,X,y):\n",
    "#         # mini batch gradient descent\n",
    "#         for i in range(0,X.shape[0],self.batch_size):\n",
    "#             end = min(X.shape[0]-1,i+self.batch_size)\n",
    "#             X_batch = X[i:end]\n",
    "#             y_batch = y[i:end]\n",
    "#             z,a = self.forward(X_batch,y_batch)\n",
    "#             loss = self.loss_fn(y_batch,a[-1])\n",
    "#             gradient_w,gradient_b = self.backpropagation(X_batch,y_batch,z,a,loss)\n",
    "    \n",
    "\n",
    "#     def loss_fn(self):\n",
    "#         pass\n",
    "\n",
    "#     def fit(self):\n",
    "#         pass\n",
    "\n",
    "#     def predict(self):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, output_size, lr=0.01, num_epoch=1000, act_fn='relu', opt_fn='SGD', \n",
    "                 n_hidden=1, n_neuron=None, batch_size=32, task_type='classification'):\n",
    "        self.lr = lr\n",
    "        self.num_epoch = num_epoch\n",
    "        self.act_fn = act_fn\n",
    "        self.opt_fn = opt_fn\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_neuron = n_neuron if n_neuron is not None else [64] * n_hidden  # Default to [64] neurons per hidden layer\n",
    "        self.batch_size = batch_size\n",
    "        self.task_type = task_type\n",
    "\n",
    "        # Initialize layer sizes (input, hidden layers, output)\n",
    "        self.layer_sizes = [input_size] + self.n_neuron + [output_size]\n",
    "        self.weights, self.biases = self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        # Initialize weights and biases for all layers\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]))\n",
    "            biases.append(np.zeros((1, self.layer_sizes[i + 1])))\n",
    "        return weights, biases\n",
    "\n",
    "    def activation_fn(self, z):\n",
    "        if self.act_fn == 'linear':\n",
    "            return z\n",
    "        elif self.act_fn == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.act_fn == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        elif self.act_fn == 'tanh':\n",
    "            return np.tanh(z)\n",
    "\n",
    "    def activation_fn_derivative(self, z):\n",
    "        if self.act_fn == 'linear':\n",
    "            return 1\n",
    "        elif self.act_fn == 'sigmoid':\n",
    "            sig = 1 / (1 + np.exp(-z))\n",
    "            return sig * (1 - sig)\n",
    "        elif self.act_fn == 'relu':\n",
    "            return (z > 0).astype(float)\n",
    "        elif self.act_fn == 'tanh':\n",
    "            return 1 - np.tanh(z) ** 2\n",
    "\n",
    "    def output_cal(self, z):\n",
    "        if self.task_type == 'classification':\n",
    "            return self.activation_fn('sigmoid')(z)\n",
    "        elif self.task_type == 'regression':\n",
    "            return z\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.a_list = []\n",
    "        self.z_list = []\n",
    "        a = X\n",
    "\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = self.activation_fn(z)\n",
    "            self.z_list.append(z)\n",
    "            self.a_list.append(a)\n",
    "\n",
    "        # Output layer\n",
    "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        if self.task_type == 'classification':\n",
    "            a = self.activation_fn(z)\n",
    "        else:\n",
    "            a = z  # For regression, no activation on the output\n",
    "        self.z_list.append(z)\n",
    "        self.a_list.append(a)\n",
    "\n",
    "        return self.a_list[-1]\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        gradients_w = []\n",
    "        gradients_b = []\n",
    "\n",
    "        # Output layer\n",
    "        if self.task_type == 'classification':\n",
    "            # delta = self.bce_loss(y,self.a_list[-1]) # Derivative of BCE loss for classification\n",
    "            delta = (self.a_list[-1] - y)  # Derivative of MSE loss for regression\n",
    "\n",
    "        else:\n",
    "            delta = (self.a_list[-1] - y) / m  # Derivative of MSE loss for regression\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dW = np.dot(self.a_list[i - 1].T, delta) / m if i != 0 else np.dot(X.T, delta) / m\n",
    "            dB = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            delta = np.dot(delta, self.weights[i].T) * self.activation_fn_derivative(self.z_list[i - 1])\n",
    "            gradients_w.append(dW)\n",
    "            gradients_b.append(dB)\n",
    "\n",
    "        gradients_w.reverse()\n",
    "        gradients_b.reverse()\n",
    "        return gradients_w, gradients_b\n",
    "\n",
    "    def update_params(self, gradients_w, gradients_b):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.lr * gradients_w[i]\n",
    "            self.biases[i] -= self.lr * gradients_b[i]\n",
    "\n",
    "    def optimizer(self, X, y):\n",
    "        if self.opt_fn == 'SGD':\n",
    "            self.sgd_optimizer(X, y)\n",
    "        elif self.opt_fn == 'BGD':\n",
    "            self.batch_optimizer(X, y)\n",
    "        elif self.opt_fn == 'MBGD':\n",
    "            self.mini_batch_optimizer(X, y)\n",
    "\n",
    "    def sgd_optimizer(self, X, y):\n",
    "        net_loss = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            X_i = X[i:i+1]\n",
    "            y_i = y[i:i+1]\n",
    "            y_hat = self.forward(X_i)\n",
    "            loss = self.bce_loss(y_i, y_hat) if self.task_type == 'classification' else self.mse_loss(y_i, y_hat)\n",
    "            net_loss += loss\n",
    "            gradients_w, gradients_b = self.backward(X_i, y_i)\n",
    "            self.update_params(gradients_w, gradients_b)\n",
    "\n",
    "        print(f'Epoch Loss: {net_loss}')\n",
    "\n",
    "    def mini_batch_optimizer(self, X, y):\n",
    "        for i in range(0, X.shape[0], self.batch_size):\n",
    "            X_batch = X[i:i + self.batch_size]\n",
    "            y_batch = y[i:i + self.batch_size]\n",
    "            y_hat = self.forward(X_batch)\n",
    "            loss = self.bce_loss(y_batch, y_hat) if self.task_type == 'classification' else self.mse_loss(y_batch, y_hat)\n",
    "            gradients_w, gradients_b = self.backward(X_batch, y_batch)\n",
    "            self.update_params(gradients_w, gradients_b)\n",
    "\n",
    "    def batch_optimizer(self, X, y):\n",
    "        y_hat = self.forward(X)\n",
    "        loss = self.bce_loss(y, y_hat) if self.task_type == 'classification' else self.mse_loss(y, y_hat)\n",
    "        gradients_w, gradients_b = self.backward(X, y)\n",
    "        self.update_params(gradients_w, gradients_b)\n",
    "\n",
    "    def bce_loss(self, y, y_hat):\n",
    "        m = y.shape[0]\n",
    "        y_hat = np.clip(y_hat, 1e-9, 1 - 1e-9)\n",
    "        return -1/m * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "    def mse_loss(self, y, y_hat):\n",
    "        m = y.shape[0]\n",
    "        return np.sum((y_hat - y) ** 2) / (2 * m)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for epoch in range(self.num_epoch):\n",
    "            self.optimizer(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X)\n",
    "        if self.task_type == 'classification':\n",
    "            return np.where(y_pred > 0.5, 1, 0)\n",
    "        else:\n",
    "            return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLP_py(nn.Module):  # Inheriting from nn.Module\n",
    "    def __init__(self, input_size, output_size, lr=0.01, num_epoch=1000,\n",
    "                 act_fn='relu', opt_fn='SGD', n_hidden=1, n_neuron=64,\n",
    "                 batch_size=32, task_type='classification'):\n",
    "        super(MLP_py, self).__init__()  # Correct usage of super for nn.Module\n",
    "        self.lr = lr\n",
    "        self.num_epoch = num_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, n_neuron))\n",
    "\n",
    "        # Adding hidden layers\n",
    "        for _ in range(n_hidden - 1):\n",
    "            self.layers.append(nn.Linear(n_neuron, n_neuron))\n",
    "\n",
    "        self.layers.append(nn.Linear(n_neuron, output_size))  # Output layer\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = nn.ReLU() if act_fn == 'relu' else nn.Sigmoid()  # Change according to your needs\n",
    "\n",
    "        # Optimizer initialization\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.layers[-1](x)  # Output layer without activation\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.train()  # Set model to training mode\n",
    "        for epoch in range(self.num_epoch):\n",
    "            inputs = torch.tensor(X_train, dtype=torch.float32)\n",
    "            targets = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "            self.optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = self.forward(inputs)  # Forward pass\n",
    "            loss = nn.BCEWithLogitsLoss()(outputs, targets)  # Binary classification loss\n",
    "            print(loss)\n",
    "            loss.backward()  # Backpropagation\n",
    "            self.optimizer.step()  # Update weights\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.forward(inputs)\n",
    "            return torch.sigmoid(outputs)  # Apply sigmoid for probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def DataPreprocessing(file_name):\n",
    "    data = pd.read_csv(file_name, index_col=0)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Select numeric columns and remove 'quality'\n",
    "    numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    numeric_columns.remove('quality')  # Remove 'quality' from the list\n",
    "    print(numeric_columns)  # Print remaining numeric columns\n",
    "    \n",
    "    # Data normalization using StandardScaler (normal standardization)\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class MLP:\n",
    "#     def __init__(self, input_size, output_size, hidden_layers, \n",
    "#                  neurons_per_layer, learning_rate, epochs, batch_size, \n",
    "#                  activation='relu', optimizer='sgd', task='classification'):\n",
    "#         self.input_size = input_size\n",
    "#         self.output_size = output_size\n",
    "#         self.hidden_layers = hidden_layers\n",
    "#         self.neurons_per_layer = neurons_per_layer\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.epochs = epochs\n",
    "#         self.batch_size = batch_size\n",
    "#         self.activation = activation\n",
    "#         self.optimizer = optimizer\n",
    "#         self.task = task  # 'classification' or 'regression'\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "        \n",
    "#         # Initialize the network\n",
    "#         self._initialize_weights()\n",
    "\n",
    "#     def _initialize_weights(self):\n",
    "#         # Randomly initialize weights and biases for each layer\n",
    "#         layer_sizes = [self.input_size] + self.neurons_per_layer + [self.output_size]\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]))\n",
    "#             self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
    "\n",
    "#     def _activation_function(self, x):\n",
    "#         if self.activation == 'relu':\n",
    "#             return np.maximum(0, x)\n",
    "#         elif self.activation == 'sigmoid':\n",
    "#             return 1 / (1 + np.exp(-x))\n",
    "#         elif self.activation == 'tanh':\n",
    "#             return np.tanh(x)\n",
    "#         elif self.activation == 'linear':\n",
    "#             return x\n",
    "\n",
    "#     def _activation_derivative(self, x):\n",
    "#         if self.activation == 'relu':\n",
    "#             return np.where(x > 0, 1, 0)\n",
    "#         elif self.activation == 'sigmoid':\n",
    "#             return x * (1 - x)\n",
    "#         elif self.activation == 'tanh':\n",
    "#             return 1 - x ** 2\n",
    "#         elif self.activation == 'linear':\n",
    "#             return 1\n",
    "\n",
    "#     def _bce_loss(self, y_true, y_pred):\n",
    "#         # Binary Cross-Entropy loss (for classification)\n",
    "#         m = y_true.shape[0]\n",
    "#         y_pred = np.clip(y_pred, 1e-9, 1 - 1e-9)  # To avoid log(0)\n",
    "#         loss = -1/m * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "#         return loss\n",
    "\n",
    "#     def _mse_loss(self, y_true, y_pred):\n",
    "#         # Mean Squared Error loss (for regression)\n",
    "#         m = y_true.shape[0]\n",
    "#         loss = np.sum((y_pred - y_true) ** 2) / (2 * m)\n",
    "#         return loss\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         self.z_values = []  # To store linear combinations\n",
    "#         self.a_values = [X]  # To store activations\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             z = np.dot(self.a_values[-1], self.weights[i]) + self.biases[i]\n",
    "#             self.z_values.append(z)\n",
    "#             # For regression task, the output layer uses a linear activation\n",
    "#             if i == len(self.weights) - 1 and self.task == 'regression':\n",
    "#                 a = z  # No activation for the output layer in regression\n",
    "#             else:\n",
    "#                 a = self._activation_function(z)\n",
    "#             self.a_values.append(a)\n",
    "        \n",
    "#         return self.a_values[-1]\n",
    "\n",
    "#     def backward(self, X, y):\n",
    "#         # Compute gradients using backpropagation\n",
    "#         m = X.shape[0]\n",
    "        \n",
    "#         if self.task == 'classification':\n",
    "#             dz = self.a_values[-1] - y  # For BCE, derivative of the loss w.r.t the output\n",
    "#         else:  # regression\n",
    "#             dz = (self.a_values[-1] - y) / m  # For MSE, derivative of the loss w.r.t the output\n",
    "        \n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             dw = np.dot(self.a_values[i].T, dz) / m\n",
    "#             db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "#             dz = np.dot(dz, self.weights[i].T) * self._activation_derivative(self.a_values[i])\n",
    "            \n",
    "#             # Update weights and biases\n",
    "#             self.weights[i] -= self.learning_rate * dw\n",
    "#             self.biases[i] -= self.learning_rate * db\n",
    "\n",
    "#     def fit(self, X_train, y_train):\n",
    "#         # Main training loop\n",
    "#         for epoch in range(self.epochs):\n",
    "#             epoch_loss = 0\n",
    "#             for i in range(0, len(X_train), self.batch_size):\n",
    "#                 X_batch = X_train[i:i + self.batch_size]\n",
    "#                 y_batch = y_train[i:i + self.batch_size]\n",
    "                \n",
    "#                 # Forward pass\n",
    "#                 y_pred = self.forward(X_batch)\n",
    "                \n",
    "#                 # Compute loss\n",
    "#                 if self.task == 'classification':\n",
    "#                     loss = self._bce_loss(y_batch, y_pred)\n",
    "#                 else:  # regression\n",
    "#                     loss = self._mse_loss(y_batch, y_pred)\n",
    "                \n",
    "#                 epoch_loss += loss\n",
    "                \n",
    "#                 # Backward pass and update\n",
    "#                 self.backward(X_batch, y_batch)\n",
    "                \n",
    "#             # Average loss per epoch\n",
    "#             epoch_loss /= len(X_train) // self.batch_size\n",
    "#             print(f'Epoch {epoch+1}/{self.epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "#     def predict(self, X_test):\n",
    "#         # Forward pass to get predictions\n",
    "#         y_pred = self.forward(X_test)\n",
    "        \n",
    "#         if self.task == 'classification':\n",
    "#             # Convert probabilities to binary class predictions\n",
    "#             return np.where(y_pred > 0.5, 1, 0)\n",
    "#         else:\n",
    "#             # For regression, return the continuous value\n",
    "#             return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'Id']\n"
     ]
    }
   ],
   "source": [
    "# Your existing imports remain the same\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# mlp = MLP(input_size=11, output_size=1, hidden_layers=2, neurons_per_layer=[64, 32], learning_rate=0.01, epochs=500, batch_size=32)\n",
    "\n",
    "# Assuming the MLP and DataPreprocessing class/method are already defined as per your code\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('WineQT.csv', index_col=0)\n",
    "data['quality'] = (data['quality'] >= 5).astype(int)  # Binary\n",
    "data.to_csv('WineQT1.csv')\n",
    "\n",
    "df = DataPreprocessing('WineQT1.csv')\n",
    "df.to_csv('final.csv')\n",
    "\n",
    "# Step 2: Split data into train/test sets\n",
    "X = df.drop(columns=['quality'])\n",
    "y = df['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays as our MLP class expects numpy inputs\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)  # Reshape to (n_samples, 1)\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)    # Reshape to (n_samples, 1)\n",
    "\n",
    "# Step 3: Initialize and train the MLP model\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "output_size = 1  # Binary classification\n",
    "mlp1 = MLP(input_size=input_size, output_size=output_size, lr=0.01, num_epoch=500, act_fn='relu', \n",
    "          opt_fn='MBGD', n_hidden=3, n_neuron=[64,64,16], batch_size=32, task_type='classification')\n",
    "\n",
    "# # Train the model\n",
    "# mlp.fit(X_train, y_train)\n",
    "\n",
    "# # Step 4: Predict and Output Metrics\n",
    "# y_train_pred = mlp.predict(X_train)\n",
    "# y_test_pred = mlp.predict(X_test)\n",
    "\n",
    "# # Assuming y_train_pred and y_test_pred contain probabilities\n",
    "# threshold = 0.5\n",
    "# y_train_pred_binary = (y_train_pred >= threshold).astype(int)  # Convert to binary\n",
    "# y_test_pred_binary = (y_test_pred >= threshold).astype(int)\n",
    "\n",
    "# # Calculate metrics\n",
    "# train_accuracy = accuracy_score(y_train.flatten(), y_train_pred_binary.flatten())\n",
    "# test_accuracy = accuracy_score(y_test.flatten(), y_test_pred_binary.flatten())\n",
    "\n",
    "# print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'Id']\n",
      "tensor(0.6130, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6103, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6076, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6050, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6023, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5997, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5971, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5945, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5919, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5893, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5867, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5842, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5816, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5791, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5766, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5741, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5716, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5691, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5666, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5641, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5617, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5592, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5568, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5544, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5520, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5496, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5472, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5448, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5424, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5401, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5377, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5354, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5331, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5308, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5284, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5261, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5239, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5216, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5193, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5170, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5148, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5125, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5103, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5081, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5058, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5036, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5014, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4992, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4971, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4949, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4927, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4906, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4884, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4863, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4841, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4820, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4799, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4778, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4757, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4736, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4715, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4694, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4674, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4653, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4633, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4612, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4592, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4572, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4551, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4531, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4511, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4491, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4471, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4451, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4432, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4412, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4392, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4373, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4353, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4334, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4315, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4295, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4276, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4257, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4238, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4219, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4200, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4182, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4163, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4144, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4126, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4107, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4089, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4071, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4052, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4034, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4016, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3998, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3980, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3962, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Train Accuracy: 0.9639\n",
      "Test Accuracy: 0.9738\n",
      "Train Accuracy: 0.0361\n",
      "Test Accuracy: 0.0262\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "# Assuming the MLP class is already defined as in the previous response\n",
    "\n",
    "# Data Preprocessing\n",
    "data = pd.read_csv('WineQT.csv', index_col=0)\n",
    "data['quality'] = (data['quality'] >= 5).astype(int)  # Binary\n",
    "data.to_csv('WineQT1.csv')\n",
    "\n",
    "df = DataPreprocessing('WineQT1.csv')\n",
    "df.to_csv('final.csv')\n",
    "\n",
    "# Split data into train/test sets\n",
    "X = df.drop(columns=['quality'])\n",
    "y = df['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays as our MLP class expects numpy inputs\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)  # Reshape to (n_samples, 1)\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)    # Reshape to (n_samples, 1)\n",
    "\n",
    "# Initialize and train the MLP model\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "output_size = 1  # Binary classification\n",
    "mlp = MLP_py(input_size=input_size, output_size=output_size, lr=0.01, num_epoch=100, \n",
    "          act_fn='relu', opt_fn='SGD', n_hidden=3, n_neuron=64, batch_size=32, task_type='classification')\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Output Metrics\n",
    "y_train_pred = mlp.predict(X_train)\n",
    "y_test_pred = mlp.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary\n",
    "threshold = 0.5\n",
    "y_train_pred_binary = (y_train_pred.numpy() >= threshold).astype(int)  # Convert to binary\n",
    "y_test_pred_binary = (y_test_pred.numpy() >= threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train.flatten(), y_train_pred_binary.flatten())\n",
    "test_accuracy = accuracy_score(y_test.flatten(), y_test_pred_binary.flatten())\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "mlp1.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Predict and Output Metrics\n",
    "y_train_pred = mlp1.predict(X_train)\n",
    "y_test_pred = mlp1.predict(X_test)\n",
    "\n",
    "# Assuming y_train_pred and y_test_pred contain probabilities\n",
    "threshold = 0.5\n",
    "y_train_pred_binary = (y_train_pred >= threshold).astype(int)  # Convert to binary\n",
    "y_test_pred_binary = (y_test_pred >= threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train.flatten(), y_train_pred_binary.flatten())\n",
    "test_accuracy = accuracy_score(y_test.flatten(), y_test_pred_binary.flatten())\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
